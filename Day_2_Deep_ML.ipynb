{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PARENT/AI-4-NICU Training School Hands-On Workshops"
      ],
      "metadata": {
        "id": "8eTHa2vFokHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lab 3. AI for medical imaging predictions"
      ],
      "metadata": {
        "id": "AWdOGcoeolsp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Install Kaggle's CLI and download the dataset"
      ],
      "metadata": {
        "id": "In82kPPyWyG9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZzc2InSgQuA"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download sovitrath/diabetic-retinopathy-224x224-gaussian-filtered"
      ],
      "metadata": {
        "id": "J_FwsfnGfbfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our time on this worshop is quite limited we'll use a [preprocessed dataset](https://www.kaggle.com/datasets/sovitrath/diabetic-retinopathy-224x224-gaussian-filtered). The gaussian kernel was applied to every image in the [source dataset](https://www.kaggle.com/c/aptos2019-blindness-detection/overview) to reduce the images size.  "
      ],
      "metadata": {
        "id": "O9vAVa95W9yw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Unzip the dataset and move it to `dataset` directory for easier access"
      ],
      "metadata": {
        "id": "SnJGlq1kX9Ye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq diabetic-retinopathy-224x224-gaussian-filtered"
      ],
      "metadata": {
        "id": "u5DxZ1z7gdb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir dataset\n",
        "!mv gaussian_filtered_images/gaussian_filtered_images/* dataset/\n",
        "!rm -rf gaussian_filtered_images"
      ],
      "metadata": {
        "id": "ZtSoDIQbg7Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Import the dataset"
      ],
      "metadata": {
        "id": "RqOhronCYHb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "02EVAaJZiDVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_data = pd.read_csv('train.csv')\n",
        "labeled_data"
      ],
      "metadata": {
        "id": "XYxTl57PhQ6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. The numerical labels are quite problematic, we don't want to learn them by heart, so let's convert them to text labels for easier interpretation"
      ],
      "metadata": {
        "id": "MsQJBtH9YO0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIAGNOSES = {\n",
        "    0: 'No_DR',\n",
        "    1: 'Mild',\n",
        "    2: 'Moderate',\n",
        "    3: 'Severe',\n",
        "    4: 'Proliferate_DR',\n",
        "}\n",
        "labeled_data['label'] = labeled_data['diagnosis'].map(DIAGNOSES)"
      ],
      "metadata": {
        "id": "IODflL3JicUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_data['label'].value_counts()"
      ],
      "metadata": {
        "id": "oNiDFX8ki02O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Our dataset contains a multilabel column. This means we should be able to train a model that can distinguish between different severities of the Diabetic Retinopaty. Let's add a binary label column in case we ever decided to train a binary classifier."
      ],
      "metadata": {
        "id": "2ncKQJc3YsLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_data['binary_label'] = labeled_data['diagnosis'].map(lambda x: \"DR\" if x != 0 else \"No_DR\")"
      ],
      "metadata": {
        "id": "o6sryyb0Z83v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_data['binary_label'].value_counts()"
      ],
      "metadata": {
        "id": "zxOopN38bFcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Let's add the image file paths to our data, for convenient access in the future steps."
      ],
      "metadata": {
        "id": "6vB76vExbJZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_data['filename'] = \"dataset/\" + labeled_data['label'] + \"/\" + labeled_data['id_code'] + \".png\""
      ],
      "metadata": {
        "id": "XGQ16Tn0olAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Let's see what exactly are we working with"
      ],
      "metadata": {
        "id": "MhtXRMAQbccO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_image(path):\n",
        "  img = cv2.imread(path)\n",
        "  display(img)"
      ],
      "metadata": {
        "id": "HDCnMjzv9DfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image in labeled_data['filename'].head():\n",
        "  display_image(image)"
      ],
      "metadata": {
        "id": "xw1yjU7b-JkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Training, validation, test division"
      ],
      "metadata": {
        "id": "prdOXkiOcG_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_test, validation = train_test_split(labeled_data, test_size = 0.15, stratify = labeled_data['label'], random_state=1)\n",
        "train, test = train_test_split(train_test, test_size = 0.15 / (1 - 0.15), stratify = train_test['label'], random_state=1)"
      ],
      "metadata": {
        "id": "C7YEOEyHk4nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def get_batches(df):\n",
        "  return ImageDataGenerator(\n",
        "      rescale = 1./255,\n",
        "      # Data augmentation\n",
        "      # zoom_range = 0.2,\n",
        "      # rotation_range = 20,\n",
        "      # shear_range = 0.2\n",
        "    ).flow_from_dataframe(\n",
        "      df,\n",
        "      x_col=\"filename\",\n",
        "      y_col=\"binary_label\",\n",
        "      # y_col=\"label\",\n",
        "      target_size=(224, 224),\n",
        "      class_mode='binary',\n",
        "      shuffle=False,\n",
        "    )\n",
        "\n",
        "train_batches = get_batches(train)\n",
        "validation_batches = get_batches(validation)\n",
        "test_batches = ImageDataGenerator(rescale = 1./255).flow_from_dataframe(\n",
        "    test,\n",
        "    x_col=\"filename\",\n",
        "    y_col=\"binary_label\",\n",
        "    # y_col=\"label\",\n",
        "    target_size=(224, 224),\n",
        "    shuffle=False,\n",
        "    class_mode='binary'\n",
        ")"
      ],
      "metadata": {
        "id": "FzeDrAn0mXup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Conv2D(8, (3,3), padding=\"valid\", input_shape=(224,224,3), activation = 'relu'),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    layers.BatchNormalization(),\n",
        "    # layers.Dropout(),\n",
        "\n",
        "    layers.Conv2D(16, (3,3), padding=\"valid\", activation = 'relu'),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    layers.BatchNormalization(),\n",
        "    # layers.Dropout(),\n",
        "\n",
        "    layers.Conv2D(32, (4,4), padding=\"valid\", activation = 'relu'),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    layers.BatchNormalization(),\n",
        "    # layers.Dropout(),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(32, activation = 'relu'),\n",
        "    # layers.Dropout(0.15),\n",
        "    layers.Dense(1, activation = 'sigmoid')\n",
        "    # layers.Dense(5, activation = 'softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-5),\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    # loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "    metrics=['acc']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(\n",
        "    train_batches,\n",
        "    epochs=30,\n",
        "    validation_data=validation_batches\n",
        ")"
      ],
      "metadata": {
        "id": "shf_sHRNnj6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To learn more about the layers used in this model see the documentation:\n",
        "* [BatchNormalization](https://keras.io/api/layers/normalization_layers/batch_normalization/)\n",
        "* [Dropout](https://keras.io/api/layers/regularization_layers/dropout/)"
      ],
      "metadata": {
        "id": "BV4x9iYRmL0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_batches)"
      ],
      "metadata": {
        "id": "zWcVRP_ZxcjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "predictions = model.predict(test_batches)\n",
        "# predicted_labels = np.argmax(predictions, axis=1)\n",
        "predicted_labels = np.rint(predictions.flatten())"
      ],
      "metadata": {
        "id": "kOHUZADtkqlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "cm = confusion_matrix(test_batches.classes, predicted_labels)\n",
        "ConfusionMatrixDisplay(cm, display_labels=test_batches.class_indices.keys()).plot()"
      ],
      "metadata": {
        "id": "1Fq1cFaVllq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transfer learning"
      ],
      "metadata": {
        "id": "jnVQN5cmpAyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q efficientnet"
      ],
      "metadata": {
        "id": "D6NrB9Mi1kCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import efficientnet.tfkeras as efn\n",
        "\n",
        "transfer_model = tf.keras.Sequential([\n",
        "  efn.EfficientNetB0(\n",
        "      input_shape=(224, 224, 3),\n",
        "      weights='imagenet',\n",
        "      include_top=False\n",
        "  ),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  # tf.keras.layers.Dense(5, activation='softmax')\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "transfer_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    #loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    metrics=['acc']\n",
        ")\n",
        "\n",
        "history = transfer_model.fit(\n",
        "    train_batches,\n",
        "    epochs=30,\n",
        "    validation_data=validation_batches\n",
        ")\n"
      ],
      "metadata": {
        "id": "_JabGhxl1oCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binary_transfer = transfer_model"
      ],
      "metadata": {
        "id": "Q25debyi6aA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transfer_model.evaluate(test_batches)"
      ],
      "metadata": {
        "id": "1gHodeHg2uV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grid search"
      ],
      "metadata": {
        "id": "ict17epho8qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LAYERS_CONFIGURATIONS = {\n",
        "    \"3 layer sets\": (\n",
        "        layers.Conv2D(16, (3,3), padding=\"valid\", activation = 'relu'),\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),\n",
        "        layers.BatchNormalization(),\n",
        "\n",
        "        layers.Conv2D(32, (4,4), padding=\"valid\", activation = 'relu'),\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),\n",
        "        layers.BatchNormalization(),\n",
        "    ),\n",
        "    \"2 layer sets\": (\n",
        "        layers.Conv2D(16, (3,3), padding=\"valid\", activation = 'relu'),\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),\n",
        "        layers.BatchNormalization(),\n",
        "    )\n",
        "}"
      ],
      "metadata": {
        "id": "I5KKH-meq413"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATES = [1e-4, 1e-5]"
      ],
      "metadata": {
        "id": "jNTPMvVrtHxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOSS_FUNCTIONS = ['categorical_crossentropy']"
      ],
      "metadata": {
        "id": "mtyO4oDatgSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(layer_configuration, learning_rate, loss_function):\n",
        "  model = tf.keras.Sequential([\n",
        "    layers.Conv2D(8, (3,3), padding=\"valid\", input_shape=(224,224,3), activation = 'relu'),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "    *layer_configuration,\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(32, activation = 'relu'),\n",
        "    #layers.Dense(5, activation = 'softmax')\n",
        "    layers.Dense(1, activation = 'sigmoid')\n",
        "  ])\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
        "      loss=loss_function,\n",
        "      metrics=['acc']\n",
        "  )\n",
        "\n",
        "  model.fit(\n",
        "      train_batches,\n",
        "      epochs=30,\n",
        "      validation_data=validation_batches\n",
        "  )\n",
        "\n",
        "  return model.evaluate(test_batches)[1]"
      ],
      "metadata": {
        "id": "FrO1vXiVp4qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "\n",
        "for layer_configuration_name, layer_configuration in LAYERS_CONFIGURATIONS.items():\n",
        "  for loss_function in LOSS_FUNCTIONS:\n",
        "    for learning_rate in LEARNING_RATES:\n",
        "      results[f\"{layer_configuration_name} x {loss_function} x {learning_rate}\"] = evaluate_model(\n",
        "          layer_configuration=layer_configuration,\n",
        "          learning_rate=learning_rate,\n",
        "          loss_function=loss_function,\n",
        "      )"
      ],
      "metadata": {
        "id": "67b28sWtCk_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "v2gdOl9nvFAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max(results.items(), key=lambda entry: entry[1])"
      ],
      "metadata": {
        "id": "RWk3-OzNvL5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving and loading the model"
      ],
      "metadata": {
        "id": "FNfnY_zpo2w5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the learning process of the deep learning models could be very lengthy it may be a good idea to store them on the disk for future use, after they are properly trained."
      ],
      "metadata": {
        "id": "Ex1XWOeWpOdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(transfer_model, open(\"model.pickle\", 'wb'))"
      ],
      "metadata": {
        "id": "GwNdgXifkiq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = pickle.load(open(\"model.pickle\", 'rb'))"
      ],
      "metadata": {
        "id": "XljOwLuykrIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "predictions = loaded_model.predict(test_batches)\n",
        "# predicted_labels = np.argmax(predictions, axis=1)\n",
        "predicted_labels = np.rint(predictions.flatten())"
      ],
      "metadata": {
        "id": "ZVZXkjAhlGIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "cm = confusion_matrix(test_batches.classes, predicted_labels)\n",
        "ConfusionMatrixDisplay(cm, display_labels=test_batches.class_indices.keys()).plot()"
      ],
      "metadata": {
        "id": "CU_SQvBLk-qJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}